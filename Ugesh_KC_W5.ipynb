{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Name: Ugesh KC\n",
        "#Student id:240088\n",
        "#UNI ID:2431328"
      ],
      "metadata": {
        "id": "z8MXfWDoy4LJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.1 Step -1- Data Understanding, Analysis and Preparations:\n",
        "In this step we will read the data, understand the data, perform some basic data cleaning, and store everything\n",
        "in the matrix as shown below.\n",
        "• Requirements:\n",
        "Dataset → student.csv\n",
        "• Decision Process:\n",
        "In this step we will define the objective of the task.\n",
        "– Objective of the Task -\n",
        "To Predict the marks obtained in writing based on the marks of Math and Reading.\n"
      ],
      "metadata": {
        "id": "IEI3V-DtYPdj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQgcEeo0YM4I",
        "outputId": "7395d816-094a-4a9d-996c-faa5f8b79a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows of the dataset:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows of the dataset:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ],
      "source": [
        "# To - Do - 1:\n",
        "# 1. Read and Observe the Dataset.\n",
        "# 2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "# 3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "# 4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "# 5. Split your data into Feature (X) and Label (Y).\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "  df = pd.read_csv('/content/drive/MyDrive/Copy of student.csv')\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: 'student.csv' not found. Please upload the file to your workspace.\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "# Print top 5 rows\n",
        "print(\"Top 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print bottom 5 rows\n",
        "print(\"\\nBottom 5 rows of the dataset:\")\n",
        "print(df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about the dataset\n",
        "print(\"\\nInformation about the dataset:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAmGPHP8Z5HU",
        "outputId": "0c4b1f47-1ea7-429c-c999-a8d9bc9a8ccd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Information about the dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print descriptive statistics\n",
        "print(\"\\nDescriptive statistics of the dataset:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8MkAv7RaNWa",
        "outputId": "1854adc9-6e1a-43f7-9005-3a5a9deec02a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descriptive statistics of the dataset:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (X) and labels (Y)\n",
        "X = df[['Math', 'Reading']]\n",
        "Y = df['Writing']\n",
        "\n",
        "print(\"\\nFeatures (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\nLabels (Y):\")\n",
        "print(Y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXS7N27OaQFt",
        "outputId": "22ff195b-00da-4434-e86e-861d678fcc1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Features (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Labels (Y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# To - Do - 2:\n",
        "# 1. To make the task easier - let’s assume there is no bias or intercept.\n",
        "# Create the following matrices:\n",
        "#    Y = W^T X\n",
        "#    Weight vector (W):\n",
        "#    W = [w1, w2, ..., wd]^T\n",
        "#    where W ∈ R^d\n",
        "\n",
        "#    Feature matrix (X):\n",
        "#    X = [\n",
        "#        [x1,1, x1,2, ..., x1,n],\n",
        "#        [x2,1, x2,2, ..., x2,n],\n",
        "#        ...,\n",
        "#        [xd,1, xd,2, ..., xd,n]\n",
        "#    ]\n",
        "#    where X ∈ R^(d×n)\n",
        "\n",
        "#    Target vector (Y):\n",
        "#    Y = [y1, y2, ..., yn]^T\n",
        "#    where Y ∈ R^n\n",
        "\n",
        "#  Note:\n",
        "#    The feature matrix (X) described above does not include a column of 1s because\n",
        "#    the model assumes the absence of a bias term.\n",
        "X_matrix = X.T.to_numpy()\n",
        "Y_matrix = Y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "W = np.random.rand(X_matrix.shape[0], 1)\n",
        "\n",
        "Y_hat = np.dot(W.T, X_matrix)\n",
        "\n",
        "print(\"Shape of Feature Matrix (X):\", X_matrix.shape)\n",
        "print(\"Shape of Weight Vector (W):\", W.shape)\n",
        "print(\"Shape of Target Vector (Y):\", Y_matrix.shape)\n",
        "print(\"Shape of Predicted Y (Y_hat):\", Y_hat.shape)\n",
        "\n",
        "print(\"\\nSample Feature Matrix (X):\")\n",
        "print(X_matrix[:, :5])\n",
        "\n",
        "print(\"\\nSample Weight Vector (W):\")\n",
        "print(W)\n",
        "\n",
        "print(\"\\nSample Target Vector (Y):\")\n",
        "print(Y_matrix[:5])\n",
        "\n",
        "print(\"\\nSample Predicted Y (Y_hat):\")\n",
        "print(Y_hat[:, :5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtWYXDR-aSq2",
        "outputId": "9f795126-9c4f-4610-8de9-fd1c524e7f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Feature Matrix (X): (2, 1000)\n",
            "Shape of Weight Vector (W): (2, 1)\n",
            "Shape of Target Vector (Y): (1000, 1)\n",
            "Shape of Predicted Y (Y_hat): (1, 1000)\n",
            "\n",
            "Sample Feature Matrix (X):\n",
            "[[48 62 79 76 59]\n",
            " [68 81 80 83 64]]\n",
            "\n",
            "Sample Weight Vector (W):\n",
            "[[0.41487224]\n",
            " [0.52954943]]\n",
            "\n",
            "Sample Target Vector (Y):\n",
            "[[63]\n",
            " [72]\n",
            " [78]\n",
            " [79]\n",
            " [62]]\n",
            "\n",
            "Sample Predicted Y (Y_hat):\n",
            "[[55.92322858 68.61558248 75.13886108 75.48289266 58.36862547]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To - Do - 3:\n",
        "# 1. Split the dataset into training and test sets.\n",
        "# 2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "# for testing.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (80/20 split)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(\"\\nShape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of Y_train:\", Y_train.shape)\n",
        "print(\"Shape of Y_test:\", Y_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7NosahBnn1V",
        "outputId": "1a74ee3f-fcc5-4720-a7cf-e93178e7cfe0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of X_train: (800, 2)\n",
            "Shape of X_test: (200, 2)\n",
            "Shape of Y_train: (800,)\n",
            "Shape of Y_test: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1.2 Step -2- Build a Cost Function:\n",
        "# Cost function is the average of the loss function measured across the data points.\n",
        "# As the cost function for a Regression problem, we will be using Mean Squared Error,\n",
        "# which is given by:\n",
        "# L(w) = (1 / 2n) * Σ [ypred(i) − yi]^2\n",
        "# where:\n",
        "# - ypred(w) = X * w\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix (d x n), where d is the number of features and n is the number of samples.\n",
        "    Y : numpy.ndarray\n",
        "        Target vector (n, ), where n is the number of samples.\n",
        "    W : numpy.ndarray\n",
        "        Weight vector (d, ), where d is the number of features.\n",
        "\n",
        "    Output:\n",
        "    cost : float\n",
        "        The accumulated mean squared error (MSE).\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the predicted values (Y_hat) using the linear model\n",
        "    Y_pred = np.dot(X, W)\n",
        "\n",
        "\n",
        "    # Step 2: Calculate the squared errors between the predicted and actual values\n",
        "    errors = Y - Y_pred\n",
        "\n",
        "    # Step 3: Compute the Mean Squared Error (MSE)\n",
        "    cost = np.mean(errors ** 2)\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "fKjcBGhHpzdn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Designing a Test Case for Cost Function:\n",
        "# We will first calculate the loss value manually and then verify the output via our code.\n",
        "# If the computed value matches, we will proceed further.\n",
        "\n",
        "# Given:\n",
        "# X =\n",
        "# [1 3 5]\n",
        "# [2 4 6]\n",
        "# Y = [3, 7, 11]\n",
        "# W = [1, 1]\n",
        "\n",
        "# The hypothesis function hθ(X) is calculated as:\n",
        "# hθ(X) = X · W\n",
        "# hθ(X) = [1 3 5] · [1 1] = [3, 7, 11]\n",
        "\n",
        "# Then, the Mean Squared Error (MSE) is calculated as:\n",
        "# J(θ) = (1 / 2n) * Σ [hθ(x^(i)) − y^(i)]^2\n",
        "# where n is the number of training examples.\n",
        "\n",
        "# Substituting the given values:\n",
        "# cost = (1 / 6) * ((3 - 3)^2 + (7 - 7)^2 + (11 - 11)^2)\n",
        "# cost = (1 / 6) * 0 = 0\n",
        "\n",
        "# Thus, for the given test case, the cost function should output: 0\n",
        "\n",
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix (d x n), where d is the number of features and n is the number of samples.\n",
        "    Y : numpy.ndarray\n",
        "        Target vector (n, ), where n is the number of samples.\n",
        "    W : numpy.ndarray\n",
        "        Weight vector (d, ), where d is the number of features.\n",
        "\n",
        "    Output:\n",
        "    cost : float\n",
        "        The accumulated mean squared error (MSE).\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the predicted values (Y_hat) using the linear model\n",
        "    Y_pred = np.dot(X, W)\n",
        "\n",
        "    # Step 2: Calculate the squared errors between the predicted and actual values\n",
        "    errors = Y - Y_pred\n",
        "\n",
        "    # Step 3: Compute the Mean Squared Error (MSE)\n",
        "    cost = np.mean(errors ** 2)\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Test case\n",
        "X_test = np.array([[1, 2],[3, 4],[5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "# Calculate the cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "\n",
        "print(\"Cost function output:\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-9IiQZvsBit",
        "outputId": "b4a853c4-0d7b-41e9-ff71-94bd66be15ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix (m x n), where m is the number of samples and n is the number of features.\n",
        "    Y : numpy.ndarray\n",
        "        Target vector (m, ), where m is the number of samples.\n",
        "    W : numpy.ndarray\n",
        "        Weight vector (n, ), where n is the number of features.\n",
        "\n",
        "    Output:\n",
        "    cost : float\n",
        "        The accumulated mean squared error (MSE).\n",
        "    \"\"\"\n",
        "    # Calculate the predicted values\n",
        "    Y_pred = np.dot(X, W)\n",
        "\n",
        "    # Calculate the squared errors\n",
        "    errors = Y - Y_pred\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE)\n",
        "    cost = np.mean(errors ** 2)\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Define the gradient descent function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix (m x n).\n",
        "    Y : numpy.ndarray\n",
        "        Target vector (m x 1).\n",
        "    W : numpy.ndarray\n",
        "        Initial guess for parameters (n x 1).\n",
        "    alpha : float\n",
        "        Learning rate.\n",
        "    iterations : int\n",
        "        Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    W_update : numpy.ndarray\n",
        "        Updated parameters (n x 1).\n",
        "    cost_history : list\n",
        "        History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = []\n",
        "\n",
        "    # Number of samples (m)\n",
        "    m = len(Y)\n",
        "\n",
        "    # Gradient descent loop\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values (hθ(X) = X * W)\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        # Step 2: Loss (Difference between predicted and actual values)\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation (dw = (2/m) * X^T * loss)\n",
        "        dw = (2/m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Step 4: Update weights (W = W - alpha * dw)\n",
        "        W_update = W - alpha * dw\n",
        "\n",
        "        # Step 5: Calculate the new cost value and store it in cost_history\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Update weights for the next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# Generate random test data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 3)\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3) # Initial guess for parameters\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De8i2BhetMz2",
        "outputId": "c19bd05e-610c-44a9-b8ca-40429b006781"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.19444407 0.46183379 0.18966481]\n",
            "Cost History: [0.2126848827535125, 0.2096956540023033, 0.2068048415077832, 0.20400913903147738, 0.2013053516475558, 0.19869039199434624, 0.1961612766520831, 0.19371512264263963, 0.19134914404713463, 0.18906064873744627, 0.186847035217795, 0.1847057895726903, 0.18263448251765826, 0.1806307665492894, 0.17869237319126174, 0.17681711033310762, 0.17500285965859919, 0.17324757416073727, 0.17154927574042497, 0.16990605288600855, 0.16831605843096223, 0.16677750738708613, 0.16528867485067233, 0.16384789397918426, 0.16245355403607245, 0.16110409850143448, 0.15979802324629896, 0.15853387476839365, 0.1573102484873253, 0.156125787097171, 0.1549791789745484, 0.15386915664029455, 0.1527944952729508, 0.1517540112723064, 0.1507465608713166, 0.149771038794766, 0.14882637696310236, 0.1479115432399203, 0.14702554022162478, 0.14616740406785345, 0.1453362033712861, 0.14453103806551396, 0.14375103836968794, 0.14299536376870614, 0.14226320202774534, 0.14155376823997828, 0.14086630390636043, 0.14020007604640541, 0.13955437633890602, 0.13892852029159206, 0.1383218464387509, 0.13773371556586855, 0.13716350996038185, 0.13661063268766171, 0.13607450689137907, 0.13555457511743044, 0.1350502986606318, 0.13456115693341256, 0.13408664685576907, 0.13362628226576237, 0.13317959334986748, 0.1327461260925062, 0.1323254417441172, 0.1319171163071394, 0.13152074003930475, 0.13113591697365853, 0.1307622644547425, 0.13039941269039848, 0.13004700431866364, 0.12970469398925247, 0.12937214795913127, 0.12904904370171244, 0.12873506952920954, 0.1284299242277091, 0.12813331670453215, 0.12784496564747053, 0.12756459919549884, 0.12729195462057505, 0.1270267780201569, 0.1267688240200725, 0.12651785548739708, 0.12627364325299867, 0.12603596584342702, 0.1258046092218313, 0.12557936653760232, 0.12536003788444572, 0.1251464300666018, 0.12493835637293835, 0.12473563635865023, 0.12453809563431101, 0.12434556566202758, 0.12415788355845969, 0.12397489190447228, 0.12379643856119844, 0.12362237649229572, 0.12345256359218824, 0.12328686252009266, 0.12312514053963314, 0.12296726936385724, 0.12281312500547094, 0.1226625876321169, 0.12251554142652575, 0.12237187445137736, 0.1222314785187121, 0.1220942490637402, 0.12196008502289998, 0.12182888871602258, 0.12170056573246475, 0.12157502482107571, 0.1214521777838696, 0.12133193937327819, 0.12121422719286327, 0.12109896160137294, 0.1209860656200281, 0.12087546484293135, 0.12076708735049248, 0.1206608636257692, 0.12055672647362525, 0.12045461094261017, 0.12035445424947021, 0.12025619570620064, 0.12015977664955493, 0.12006514037292691, 0.11997223206052741, 0.11988099872377651, 0.11979138913983835, 0.11970335379222512, 0.11961684481340114, 0.11953181592931976, 0.1194482224058277, 0.11936602099687374, 0.11928516989446196, 0.11920562868028944, 0.11912735827901288, 0.11905032091308854, 0.11897448005913265, 0.11889980040575163, 0.11882624781279176, 0.11875378927196112, 0.11868239286877731, 0.1186120277457966, 0.11854266406708089, 0.11847427298386112, 0.11840682660135701, 0.11834029794671358, 0.11827466093801751, 0.1182098903543565, 0.11814596180688693, 0.11808285171087517, 0.11802053725868075, 0.11795899639364883, 0.11789820778488158, 0.11783815080285912, 0.11777880549588116, 0.11772015256730184, 0.11766217335353076, 0.11760484980277494, 0.11754816445449634, 0.11749210041956072, 0.1174366413610552, 0.11738177147575123, 0.11732747547619192, 0.11727373857338211, 0.1172205464600613, 0.11716788529453953, 0.11711574168507768, 0.11706410267479336, 0.11701295572707472, 0.11696228871148572, 0.11691208989014551, 0.11686234790456622, 0.11681305176293418, 0.11676419082781883, 0.11671575480429568, 0.11666773372846868, 0.11662011795637932, 0.11657289815328868, 0.11652606528332032, 0.11647961059945189, 0.11643352563384349, 0.11638780218849128, 0.11634243232619607, 0.11629740836183569, 0.11625272285393087, 0.11620836859649525, 0.11616433861115942, 0.11612062613956009, 0.11607722463598552, 0.11603412776026807, 0.11599132937091618, 0.11594882351847746, 0.11590660443912483, 0.1158646665484588, 0.11582300443551802, 0.11578161285699144, 0.1157404867316251, 0.11569962113481726, 0.11565901129339527, 0.11561865258056807, 0.11557854051104847, 0.11553867073633972, 0.11549903904018043, 0.11545964133414272, 0.11542047365337836, 0.11538153215250838, 0.11534281310165012, 0.11530431288257889, 0.11526602798501787, 0.115227955003053, 0.11519009063166889, 0.11515243166340053, 0.11511497498509789, 0.11507771757479934, 0.11504065649871026, 0.1150037889082827, 0.11496711203739395, 0.11493062319961941, 0.11489431978559729, 0.11485819926048198, 0.11482225916148278, 0.11478649709548555, 0.11475091073675422, 0.1147154978247094, 0.11468025616178183, 0.11464518361133783, 0.11461027809567442, 0.11457553759408189, 0.11454096014097125, 0.1145065438240649, 0.11447228678264759, 0.11443818720587665, 0.11440424333114853, 0.11437045344252034, 0.11433681586918444, 0.1143033289839941, 0.11426999120203883, 0.11423680097926724, 0.11420375681115641, 0.1141708572314257, 0.11413810081079387, 0.11410548615577772, 0.1140730119075311, 0.11404067674072298, 0.1140084793624529, 0.11397641851120299, 0.11394449295582505, 0.11391270149456148, 0.11388104295409919, 0.11384951618865514, 0.1138181200790926, 0.1137868535320667, 0.11375571547919903, 0.11372470487627956, 0.11369382070249555, 0.11366306195968592, 0.11363242767162095, 0.11360191688330583, 0.11357152866030754, 0.11354126208810446, 0.11351111627145738, 0.11348109033380183, 0.1134511834166606, 0.11342139467907604, 0.1133917232970608, 0.11336216846306785, 0.1133327293854773, 0.11330340528810084, 0.1132741954097026, 0.11324509900353603, 0.11321611533689614, 0.11318724369068671, 0.11315848335900207, 0.11312983364872259, 0.11310129387912381, 0.11307286338149856, 0.11304454149879156, 0.1130163275852463, 0.11298822100606365, 0.11296022113707185, 0.11293232736440723, 0.11290453908420602, 0.11287685570230592, 0.11284927663395779, 0.11282180130354699, 0.1127944291443238, 0.11276715959814294, 0.11273999211521141, 0.1127129261538452, 0.11268596118023363, 0.11265909666821161, 0.11263233209903936, 0.11260566696118984, 0.11257910075014241, 0.11255263296818416, 0.11252626312421711, 0.11249999073357228, 0.11247381531782946, 0.11244773640464328, 0.11242175352757483, 0.11239586622592868, 0.1123700740445956, 0.11234437653390017, 0.11231877324945354, 0.11229326375201085, 0.11226784760733358, 0.1122425243860561, 0.11221729366355693, 0.11219215501983378, 0.11216710803938323, 0.11214215231108371, 0.11211728742808277, 0.11209251298768791, 0.11206782859126074, 0.11204323384411481, 0.11201872835541674, 0.11199431173809046, 0.11196998360872469, 0.11194574358748306, 0.11192159129801782, 0.11189752636738547, 0.11187354842596578, 0.11184965710738323, 0.11182585204843078, 0.11180213288899628, 0.11177849927199128, 0.11175495084328185, 0.11173148725162209, 0.11170810814858907, 0.11168481318852058, 0.11166160202845435, 0.11163847432806935, 0.11161542974962904, 0.11159246795792627, 0.11156958862022998, 0.11154679140623376, 0.11152407598800557, 0.11150144203993945, 0.11147888923870879, 0.1114564172632205, 0.11143402579457146, 0.11141171451600534, 0.11138948311287176, 0.11136733127258591, 0.11134525868458983, 0.11132326504031483, 0.111301350033145, 0.11127951335838189, 0.11125775471321012, 0.11123607379666436, 0.11121447030959693, 0.11119294395464666, 0.11117149443620855, 0.11115012146040443, 0.11112882473505444, 0.11110760396964935, 0.11108645887532384, 0.1110653891648303, 0.11104439455251372, 0.11102347475428712, 0.11100262948760772, 0.1109818584714539, 0.11096116142630282, 0.11094053807410845, 0.11091998813828066, 0.11089951134366459, 0.1108791074165206, 0.11085877608450508, 0.1108385170766514, 0.1108183301233519, 0.11079821495633978, 0.1107781713086721, 0.1107581989147128, 0.11073829751011642, 0.11071846683181215, 0.11069870661798852, 0.11067901660807822, 0.11065939654274354, 0.11063984616386209, 0.1106203652145131, 0.1106009534389637, 0.11058161058265588, 0.11056233639219389, 0.11054313061533153, 0.11052399300096015, 0.11050492329909685, 0.11048592126087291, 0.11046698663852256, 0.11044811918537213, 0.11042931865582933, 0.1104105848053728, 0.11039191739054204, 0.11037331616892748, 0.11035478089916084, 0.11033631134090567, 0.11031790725484827, 0.11029956840268845, 0.11028129454713104, 0.11026308545187714, 0.11024494088161578, 0.11022686060201581, 0.11020884437971791, 0.11019089198232652, 0.11017300317840259, 0.11015517773745577, 0.11013741542993735, 0.1101197160272328, 0.11010207930165512, 0.11008450502643767, 0.1100669929757276, 0.11004954292457934, 0.11003215464894796, 0.11001482792568304, 0.10999756253252235, 0.10998035824808587, 0.10996321485186983, 0.10994613212424076, 0.10992910984642996, 0.10991214780052776, 0.10989524576947791, 0.10987840353707243, 0.10986162088794614, 0.10984489760757132, 0.10982823348225286, 0.10981162829912292, 0.10979508184613634, 0.10977859391206529, 0.109762164286495, 0.10974579275981867, 0.10972947912323297, 0.10971322316873357, 0.10969702468911048, 0.10968088347794376, 0.10966479932959915, 0.10964877203922374, 0.10963280140274176, 0.10961688721685052, 0.10960102927901615, 0.10958522738746974, 0.10956948134120323, 0.10955379093996548, 0.10953815598425849, 0.1095225762753336, 0.10950705161518752, 0.10949158180655878, 0.10947616665292409, 0.10946080595849461, 0.10944549952821234, 0.10943024716774673, 0.10941504868349108, 0.10939990388255913, 0.10938481257278156, 0.10936977456270272, 0.10935478966157733, 0.10933985767936705, 0.1093249784267373, 0.10931015171505398, 0.10929537735638045, 0.10928065516347406, 0.10926598494978342, 0.1092513665294449, 0.10923679971727987, 0.1092222843287916, 0.10920782018016215, 0.10919340708824953, 0.10917904487058464, 0.10916473334536843, 0.10915047233146902, 0.10913626164841882, 0.10912210111641159, 0.10910799055629986, 0.10909392978959186, 0.10907991863844897, 0.10906595692568292, 0.10905204447475288, 0.10903818110976317, 0.1090243666554602, 0.10901060093722997, 0.10899688378109552, 0.10898321501371405, 0.10896959446237471, 0.10895602195499565, 0.10894249732012176, 0.10892902038692187, 0.10891559098518647, 0.10890220894532508, 0.10888887409836379, 0.10887558627594274, 0.10886234531031383, 0.10884915103433813, 0.10883600328148355, 0.1088229018858223, 0.10880984668202881, 0.108796837505377, 0.10878387419173814, 0.10877095657757853, 0.10875808449995696, 0.10874525779652267, 0.1087324763055129, 0.10871973986575062, 0.1087070483166423, 0.10869440149817561, 0.10868179925091712, 0.10866924141601032, 0.10865672783517309, 0.10864425835069567, 0.10863183280543848, 0.10861945104282977, 0.1086071129068637, 0.10859481824209796, 0.10858256689365176, 0.1085703587072037, 0.10855819352898949, 0.10854607120580001, 0.10853399158497914, 0.10852195451442173, 0.10850995984257135, 0.10849800741841842, 0.10848609709149805, 0.10847422871188801, 0.1084624021302067, 0.10845061719761111, 0.10843887376579478, 0.1084271716869858, 0.1084155108139449, 0.10840389099996331, 0.10839231209886092, 0.10838077396498419, 0.10836927645320425, 0.10835781941891495, 0.10834640271803091, 0.10833502620698551, 0.10832368974272916, 0.10831239318272709, 0.10830113638495784, 0.10828991920791084, 0.10827874151058499, 0.1082676031524866, 0.10825650399362735, 0.10824544389452269, 0.10823442271618987, 0.10822344032014602, 0.10821249656840638, 0.10820159132348253, 0.10819072444838047, 0.10817989580659872, 0.10816910526212664, 0.10815835267944268, 0.10814763792351244, 0.10813696085978691, 0.10812632135420074, 0.10811571927317042, 0.10810515448359254, 0.10809462685284206, 0.1080841362487704, 0.10807368253970397, 0.10806326559444218, 0.1080528852822558, 0.10804254147288525, 0.10803223403653892, 0.10802196284389137, 0.10801172776608164, 0.10800152867471162, 0.10799136544184426, 0.10798123794000208, 0.1079711460421652, 0.10796108962176997, 0.10795106855270702, 0.10794108270931989, 0.10793113196640312, 0.10792121619920086, 0.10791133528340495, 0.10790148909515354, 0.10789167751102932, 0.10788190040805799, 0.1078721576637065, 0.10786244915588167, 0.10785277476292837, 0.10784313436362807, 0.10783352783719719, 0.10782395506328557, 0.10781441592197481, 0.10780491029377673, 0.1077954380596319, 0.10778599910090803, 0.10777659329939819, 0.1077672205373198, 0.10775788069731247, 0.10774857366243694, 0.10773929931617329, 0.10773005754241956, 0.10772084822549008, 0.10771167125011413, 0.10770252650143444, 0.10769341386500532, 0.10768433322679181, 0.10767528447316767, 0.10766626749091401, 0.10765728216721794, 0.10764832838967106, 0.10763940604626793, 0.10763051502540465, 0.10762165521587735, 0.10761282650688092, 0.10760402878800723, 0.10759526194924413, 0.10758652588097359, 0.10757782047397063, 0.10756914561940155, 0.10756050120882295, 0.10755188713417978, 0.10754330328780441, 0.10753474956241497, 0.1075262258511141, 0.10751773204738738, 0.10750926804510214, 0.10750083373850583, 0.10749242902222494, 0.10748405379126348, 0.1074757079410016, 0.1074673913671942, 0.1074591039659697, 0.10745084563382858, 0.10744261626764212, 0.10743441576465093, 0.10742624402246381, 0.10741810093905613, 0.10740998641276893, 0.1074019003423071, 0.10739384262673844, 0.10738581316549223, 0.10737781185835786, 0.10736983860548369, 0.1073618933073755, 0.10735397586489545, 0.10734608617926066, 0.10733822415204199, 0.10733038968516263, 0.10732258268089709, 0.10731480304186956, 0.10730705067105306, 0.10729932547176783, 0.1072916273476803, 0.10728395620280176, 0.10727631194148703, 0.1072686944684334, 0.10726110368867926, 0.10725353950760294, 0.10724600183092138, 0.10723849056468904, 0.10723100561529657, 0.10722354688946972, 0.10721611429426801, 0.10720870773708352, 0.10720132712563984, 0.10719397236799072, 0.10718664337251893, 0.1071793400479351, 0.10717206230327647, 0.1071648100479058, 0.10715758319151013, 0.10715038164409962, 0.1071432053160065, 0.10713605411788352, 0.1071289279607034, 0.1071218267557572, 0.10711475041465329, 0.1071076988493163, 0.10710067197198589, 0.10709366969521562, 0.10708669193187195, 0.10707973859513283, 0.10707280959848685, 0.10706590485573204, 0.10705902428097465, 0.10705216778862812, 0.10704533529341205, 0.10703852671035102, 0.10703174195477333, 0.1070249809423102, 0.10701824358889452, 0.10701152981075979, 0.10700483952443894, 0.10699817264676344, 0.1069915290948621, 0.10698490878615997, 0.1069783116383774, 0.10697173756952878, 0.10696518649792171, 0.10695865834215582, 0.10695215302112157, 0.10694567045399964, 0.10693921056025932, 0.10693277325965794, 0.10692635847223958, 0.10691996611833407, 0.10691359611855603, 0.10690724839380385, 0.1069009228652585, 0.10689461945438275, 0.10688833808291996, 0.10688207867289316, 0.1068758411466041, 0.10686962542663211, 0.10686343143583316, 0.10685725909733886, 0.10685110833455556, 0.10684497907116317, 0.1068388712311143, 0.10683278473863334, 0.10682671951821529, 0.10682067549462498, 0.10681465259289592, 0.10680865073832953, 0.10680266985649398, 0.10679670987322332, 0.10679077071461661, 0.10678485230703676, 0.10677895457710976, 0.10677307745172367, 0.10676722085802766, 0.10676138472343105, 0.10675556897560246, 0.10674977354246876, 0.1067439983522143, 0.10673824333327976, 0.10673250841436141, 0.10672679352441022, 0.10672109859263072, 0.1067154235484802, 0.10670976832166801, 0.10670413284215423, 0.10669851704014921, 0.10669292084611236, 0.10668734419075127, 0.10668178700502107, 0.10667624922012324, 0.10667073076750491, 0.10666523157885789, 0.10665975158611783, 0.10665429072146336, 0.10664884891731514, 0.10664342610633501, 0.10663802222142518, 0.10663263719572738, 0.10662727096262183, 0.10662192345572659, 0.10661659460889662, 0.10661128435622279, 0.10660599263203138, 0.10660071937088281, 0.10659546450757108, 0.10659022797712293, 0.10658500971479676, 0.10657980965608203, 0.10657462773669849, 0.10656946389259503, 0.10656431805994908, 0.10655919017516585, 0.10655408017487733, 0.10654898799594165, 0.10654391357544199, 0.10653885685068616, 0.10653381775920545, 0.10652879623875401, 0.10652379222730804, 0.10651880566306486, 0.1065138364844423, 0.10650888463007772, 0.10650395003882739, 0.10649903264976562, 0.10649413240218396, 0.1064892492355904, 0.10648438308970867, 0.10647953390447745, 0.10647470162004953, 0.10646988617679115, 0.10646508751528108, 0.10646030557630998, 0.10645554030087963, 0.10645079163020203, 0.10644605950569895, 0.10644134386900074, 0.10643664466194604, 0.10643196182658059, 0.10642729530515699, 0.10642264504013338, 0.10641801097417314, 0.10641339305014404, 0.10640879121111738, 0.10640420540036737, 0.10639963556137044, 0.10639508163780435, 0.10639054357354771, 0.10638602131267895, 0.1063815147994759, 0.10637702397841484, 0.10637254879417, 0.10636808919161267, 0.10636364511581049, 0.10635921651202698, 0.10635480332572059, 0.10635040550254393, 0.10634602298834349, 0.10634165572915849, 0.1063373036712204, 0.10633296676095227, 0.10632864494496796, 0.10632433817007149, 0.1063200463832563, 0.10631576953170477, 0.10631150756278718, 0.10630726042406145, 0.10630302806327215, 0.10629881042834993, 0.10629460746741098, 0.10629041912875613, 0.10628624536087035, 0.10628208611242203, 0.1062779413322624, 0.10627381096942468, 0.10626969497312368, 0.10626559329275494, 0.1062615058778942, 0.10625743267829671, 0.1062533736438966, 0.10624932872480615, 0.1062452978713154, 0.10624128103389115, 0.10623727816317662, 0.10623328920999066, 0.10622931412532725, 0.10622535286035469, 0.10622140536641518, 0.106217471595024, 0.10621355149786899, 0.10620964502681, 0.10620575213387809, 0.1062018727712751, 0.10619800689137289, 0.10619415444671286, 0.10619031539000519, 0.10618648967412843, 0.10618267725212867, 0.10617887807721914, 0.10617509210277948, 0.1061713192823552, 0.10616755956965701, 0.10616381291856042, 0.10616007928310488, 0.10615635861749345, 0.10615265087609198, 0.10614895601342866, 0.1061452739841935, 0.10614160474323757, 0.10613794824557252, 0.10613430444637004, 0.10613067330096125, 0.1061270547648361, 0.10612344879364272, 0.10611985534318713, 0.10611627436943243, 0.1061127058284983, 0.1061091496766604, 0.10610560587034987, 0.10610207436615282, 0.10609855512080965, 0.10609504809121453, 0.10609155323441496, 0.10608807050761104, 0.10608459986815509, 0.10608114127355096, 0.10607769468145363, 0.10607426004966854, 0.10607083733615115, 0.10606742649900625, 0.10606402749648768, 0.10606064028699753, 0.10605726482908581, 0.10605390108144976, 0.10605054900293343, 0.10604720855252711, 0.10604387968936685, 0.10604056237273385, 0.10603725656205398, 0.10603396221689736, 0.10603067929697765, 0.10602740776215175, 0.10602414757241903, 0.10602089868792117, 0.10601766106894123, 0.10601443467590352, 0.10601121946937277, 0.10600801541005403, 0.10600482245879168, 0.10600164057656933, 0.10599846972450906, 0.10599530986387115, 0.10599216095605342, 0.10598902296259072, 0.10598589584515448, 0.10598277956555244, 0.10597967408572777, 0.10597657936775887, 0.10597349537385871, 0.1059704220663745, 0.1059673594077871, 0.10596430736071072, 0.10596126588789202, 0.10595823495221017, 0.10595521451667604, 0.10595220454443177, 0.10594920499875043, 0.10594621584303533, 0.10594323704081987, 0.10594026855576665, 0.1059373103516675, 0.10593436239244257, 0.10593142464214013, 0.1059284970649361, 0.10592557962513345, 0.10592267228716185, 0.10591977501557723, 0.10591688777506131, 0.10591401053042106, 0.10591114324658837, 0.10590828588861959, 0.105905438421695, 0.1059026008111184, 0.1058997730223168, 0.10589695502083978, 0.10589414677235906, 0.10589134824266838, 0.10588855939768249, 0.10588578020343736, 0.10588301062608925, 0.1058802506319145, 0.10587750018730911, 0.10587475925878824, 0.10587202781298576, 0.10586930581665392, 0.10586659323666288, 0.1058638900400003, 0.10586119619377085, 0.10585851166519596, 0.10585583642161313, 0.10585317043047583, 0.10585051365935279, 0.10584786607592785, 0.10584522764799935, 0.10584259834347978, 0.10583997813039545, 0.10583736697688602, 0.10583476485120398, 0.10583217172171454, 0.10582958755689488, 0.105827012325334, 0.10582444599573218, 0.10582188853690071, 0.10581933991776137, 0.10581680010734608, 0.10581426907479655, 0.10581174678936378, 0.10580923322040782, 0.10580672833739718, 0.10580423210990872, 0.105801744507627, 0.10579926550034392, 0.10579679505795854, 0.1057943331504765, 0.10579187974800977, 0.10578943482077613, 0.10578699833909887, 0.10578457027340649, 0.10578215059423213, 0.1057797392722134, 0.10577733627809191, 0.10577494158271283, 0.10577255515702463, 0.10577017697207877, 0.10576780699902903, 0.10576544520913153, 0.10576309157374408, 0.10576074606432596, 0.10575840865243755, 0.10575607930973988, 0.10575375800799433, 0.10575144471906234, 0.10574913941490476, 0.10574684206758202, 0.10574455264925325, 0.10574227113217625, 0.10573999748870692, 0.10573773169129916, 0.10573547371250427, 0.10573322352497082, 0.10573098110144402, 0.10572874641476572, 0.10572651943787385, 0.10572430014380199, 0.10572208850567939]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1.4 Step -4- Evaluate the Model:\n",
        "# Evaluation in Machine Learning measures the goodness of fit of your built model.\n",
        "# Let's see how good the model we designed above is. As discussed in class for regression,\n",
        "# we can use the following function as an evaluation measure.\n",
        "# 1. Root Mean Square Error (RMSE):\n",
        "# The Root Mean Squared Error (RMSE) is a commonly used metric for measuring the average magnitude of\n",
        "# the errors between predicted and actual values. It is given by the following formula:\n",
        "#\n",
        "# RMSE = sqrt( (1 / n) * Σ (yi − y_hat(i))^2 )\n",
        "#\n",
        "# Where:\n",
        "# - n is the number of samples,\n",
        "# - yi is the actual value of the i-th sample,\n",
        "# - y_hat(i) is the predicted value of the i-th sample.\n",
        "\n",
        "\n",
        "\n",
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the Root Mean Squared Error (RMSE).\n",
        "\n",
        "    Parameters:\n",
        "    Y : numpy.ndarray\n",
        "        Array of actual (target) dependent variables (m, ).\n",
        "    Y_pred : numpy.ndarray\n",
        "        Array of predicted dependent variables (m, ).\n",
        "\n",
        "    Returns:\n",
        "    rmse : float\n",
        "        The root mean squared error.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the squared differences between actual and predicted values\n",
        "    squared_differences = (Y - Y_pred) ** 2\n",
        "\n",
        "    # Step 2: Calculate the mean of the squared differences\n",
        "    mean_squared_error = np.mean(squared_differences)\n",
        "\n",
        "    # Step 3: Take the square root of the mean squared error\n",
        "    rmse_value = np.sqrt(mean_squared_error)\n",
        "\n",
        "    return rmse_value\n",
        "\n"
      ],
      "metadata": {
        "id": "wHS5iyeAtxLt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model Evaluation - R-squared\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the R Squared value, which measures the goodness of fit.\n",
        "\n",
        "    Parameters:\n",
        "    Y : numpy.ndarray\n",
        "        Array of actual (target) dependent variables (m, ).\n",
        "    Y_pred : numpy.ndarray\n",
        "        Array of predicted dependent variables (m, ).\n",
        "\n",
        "    Returns:\n",
        "    r2 : float\n",
        "        The R-squared value.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the mean of the actual values (Y)\n",
        "    mean_y = np.mean(Y)\n",
        "\n",
        "    # Step 2: Calculate the Total Sum of Squares (SST)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "\n",
        "    # Step 3: Calculate the Sum of Squared Residuals (SSR)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    # Step 4: Calculate the R-squared value\n",
        "    r2_value = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    return r2_value\n"
      ],
      "metadata": {
        "id": "IMtKkzWsuqXE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gradient Descent Function (as you wrote earlier)\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W)\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1/m) * np.dot(X.T, loss)\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W -= alpha * dw\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "# Cost Function\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1 / (2 * m)) * np.sum(np.square(Y_pred - Y))\n",
        "    return cost\n",
        "\n",
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "# Model Evaluation - R2\n",
        "def r2(Y, Y_pred):\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/Copy of student.csv\")\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])\n",
        "    alpha = 0.00001\n",
        "    iterations = 1000\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htudImb1uwTZ",
        "outputId": "5d3b219f-ad19-4466-c598-d000c2ecc0bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [2013.165570783755, 1640.286832599692, 1337.0619994901588, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951635, 398.7634463599484, 327.4517147324688]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to calculate Cost (Mean Squared Error)\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1/(2*m)) * np.sum((Y_pred - Y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# Gradient Descent function with Gradient Clipping\n",
        "def gradient_descent(X, Y, W, alpha, iterations, clip_threshold=1.0):\n",
        "    cost_history = []\n",
        "    m = len(Y)\n",
        "    for _ in range(iterations):\n",
        "        Y_pred = np.dot(X, W)\n",
        "        loss = Y_pred - Y\n",
        "        dw = (1/m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        dw_norm = np.linalg.norm(dw)\n",
        "        if dw_norm > clip_threshold:\n",
        "            dw = dw * clip_threshold / dw_norm\n",
        "\n",
        "        W = W - alpha * dw\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history.append(cost)\n",
        "    return W, cost_history\n",
        "\n",
        "# Root Mean Squared Error (RMSE)\n",
        "def rmse(Y, Y_pred):\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "# R-Squared function\n",
        "def r2(Y, Y_pred):\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Main Function to integrate all steps\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/Copy of student.csv\")\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values\n",
        "\n",
        "    # Split the dataset into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Feature scaling using StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Initialize weights, number of iterations, and learning rates\n",
        "    W_initial = np.zeros(X_train.shape[1])\n",
        "    iterations = 1000\n",
        "    learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "    for alpha in learning_rates:\n",
        "        print(f\"Experimenting with Learning Rate: {alpha}\")\n",
        "\n",
        "        # Train the model using Gradient Descent with Gradient Clipping\n",
        "        W_optimal, cost_history = gradient_descent(X_train, Y_train, W_initial, alpha, iterations)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "        # Evaluate the model\n",
        "        model_rmse = rmse(Y_test, Y_pred)\n",
        "        model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "        print(f\"RMSE: {model_rmse:.4f}\")\n",
        "        print(f\"R-squared: {model_r2:.4f}\")\n",
        "\n",
        "        # Check for overfitting/underfitting\n",
        "        if model_rmse > 10:\n",
        "            print(\"Model might be underfitting.\")\n",
        "        elif model_rmse < 2 and model_r2 < 0.6:\n",
        "            print(\"Model might be overfitting.\")\n",
        "        else:\n",
        "            print(\"Model performance seems acceptable.\")\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rO3uvZXvLrB",
        "outputId": "380d71bf-e3d6-4f4a-8ea1-05ef8444eea3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimenting with Learning Rate: 0.0001\n",
            "RMSE: 70.1710\n",
            "R-squared: -18.6709\n",
            "Model might be underfitting.\n",
            "------------------------------\n",
            "Experimenting with Learning Rate: 0.001\n",
            "RMSE: 69.9576\n",
            "R-squared: -18.5514\n",
            "Model might be underfitting.\n",
            "------------------------------\n",
            "Experimenting with Learning Rate: 0.01\n",
            "RMSE: 69.1647\n",
            "R-squared: -18.1107\n",
            "Model might be underfitting.\n",
            "------------------------------\n",
            "Experimenting with Learning Rate: 0.1\n",
            "RMSE: 69.4253\n",
            "R-squared: -18.2550\n",
            "Model might be underfitting.\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B_rqlwdzwysa"
      }
    }
  ]
}